This folder contains the backend chatbot system used in our Graduation Project "Stress Detection Using Machine Learning Techniques".

The chatbot provides short, controlled responses and explains stress-related metrics such as:

- Body Temperature  
- Electrodermal Activity (EDA)  
- Blood Volume Pulse (BVP)  
- Movement Activity  

---

## Tech Stack

- Node.js  
- Express.js  
- Ollama (local LLM engine)  
- Llama 3.2 model  

---

## How It Works

**Note:**  
This backend handles *only* the chatbot logic and LLM responses.  
The frontend (chat interface) is managed by another team member and communicates with this backend through **HTTP POST → /chat**.

The server receives a user message → builds a strict prompt → sends it to Ollama Llama 3.2 → gets a short controlled reply → returns it to the client.

---

## Files Included

- **server.js** → Main backend server  
- **package.json** → Project dependencies & scripts  
- **package-lock.json** → Dependency lock file  

---

## Run Locally

1. Make sure Node.js is installed  
2. Install dependencies:  
   ```
   npm install
   ```
3. Start the server:  
   ```
   node server.js
   ```
4. Make sure Ollama is installed and Llama 3.2 model is available:  
   ```
   ollama run llama3.2
   ```
5. The backend will run at:  
   ```
   http://localhost:3000
   ```

---

## API Endpoint

### **POST /chat**

Sends a user message to the backend and returns a short controlled chatbot reply.

#### Request Body (JSON):
```json
{
  "message": "Hello"
}
```

#### Response Body (JSON):
```json
{
  "reply": "Hello! How can I help you today?"
}
```
